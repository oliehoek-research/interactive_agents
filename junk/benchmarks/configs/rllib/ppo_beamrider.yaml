PPO_BeamRider:
  run: PPO
  env: BeamRiderNoFrameskip-v4
  stop:
    timesteps_total: 2000000
  checkpoint_freq: 500
  checkpoint_at_end: True
  num_samples: 2
  config:
    lambda: 0.95
    kl_coeff: 0.5
    clip_rewards: True
    clip_param: 0.1
    entropy_coeff: 0.01
    train_batch_size: 2000
    rollout_fragment_length: 100
    sgd_minibatch_size: 500
    num_sgd_iter: 5
    num_workers: 4
    num_envs_per_worker: 5
    # DEFAULTS
    gamma: 0.99
    lr: 5.e-5
    use_critic: True
    use_gae: True
    shuffle_sequences: True
    vf_loss_coeff: 1.0
    vf_clip_param: 10.0
    grad_clip: null
    kl_target: 0.01
    batch_mode: truncate_episodes
    observation_filter: NoFilter
    # MODEL
    model:
      vf_share_layers: True
      # DEFAULTS
      fcnet_hiddens: [256, 256]
      fcnet_activation: tanh
      conv_filters: null
      conv_activation: relu